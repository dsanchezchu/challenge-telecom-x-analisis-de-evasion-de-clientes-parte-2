{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ TelecomX - Modelos Predictivos (Parte 2)\n",
        "---\n",
        "## üéØ Objetivo: Predicci√≥n de Churn y An√°lisis Avanzado\n",
        "Este notebook contin√∫a el proyecto ETL implementando modelos de Machine Learning para predecir el abandono de clientes (churn) y generar insights accionables."
      ],
      "metadata": {
        "id": "header_section"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Carga de Datos Procesados"
      ],
      "metadata": {
        "id": "data_loading"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librer√≠as necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Configuraci√≥n de visualizaci√≥n\n",
        "plt.style.use('default')\n",
        "sns.set_palette('viridis')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"üöÄ Librer√≠as cargadas exitosamente\")\n",
        "print(\"üìà Iniciando Parte 2: Modelos Predictivos\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funci√≥n para cargar datos procesados\n",
        "def load_processed_data():\n",
        "    try:\n",
        "        # Intentar cargar datos del CSV de la Parte 1\n",
        "        df = pd.read_csv('telecom_data_processed.csv')\n",
        "        print(f\"‚úÖ Datos cargados desde CSV: {len(df)} registros\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ö†Ô∏è Archivo CSV no encontrado. Generando datos simulados...\")\n",
        "        # Si no existe el CSV, generar datos similares a la Parte 1\n",
        "        return generate_sample_data()\n",
        "\n",
        "def generate_sample_data():\n",
        "    \"\"\"Generar datos simulados similares a la Parte 1 para continuidad\"\"\"\n",
        "    np.random.seed(42)\n",
        "    n_records = 1000\n",
        "    \n",
        "    data = {\n",
        "        'customer_id': range(1, n_records + 1),\n",
        "        'plan_type': np.random.choice(['B√°sico', 'Premium', 'Enterprise'], n_records, p=[0.5, 0.3, 0.2]),\n",
        "        'monthly_charges': np.random.normal(45, 15, n_records),\n",
        "        'total_charges': np.random.normal(500, 200, n_records),\n",
        "        'tenure_months': np.random.randint(1, 72, n_records),\n",
        "        'data_usage_gb': np.random.exponential(10, n_records),\n",
        "        'voice_minutes': np.random.poisson(300, n_records),\n",
        "        'sms_count': np.random.poisson(50, n_records),\n",
        "        'region': np.random.choice(['Norte', 'Sur', 'Centro', 'Este', 'Oeste'], n_records),\n",
        "        'churn': np.random.choice([0, 1], n_records, p=[0.8, 0.2]),\n",
        "        'signup_date': pd.date_range(start='2020-01-01', end='2024-12-31', periods=n_records)\n",
        "    }\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    \n",
        "    # Aplicar transformaciones b√°sicas (simulando ETL de Parte 1)\n",
        "    df['monthly_charges'] = df['monthly_charges'].round(2)\n",
        "    df['total_charges'] = df['total_charges'].round(2)\n",
        "    df['data_usage_gb'] = df['data_usage_gb'].round(2)\n",
        "    df['revenue_per_month'] = df['total_charges'] / df['tenure_months']\n",
        "    df['signup_year'] = df['signup_date'].dt.year\n",
        "    df['customer_segment'] = pd.cut(df['monthly_charges'], bins=3, labels=['Bajo', 'Medio', 'Alto'])\n",
        "    \n",
        "    # Filtrar valores positivos\n",
        "    df = df[df['monthly_charges'] > 0]\n",
        "    df = df[df['total_charges'] > 0]\n",
        "    \n",
        "    print(f\"üìä Datos simulados generados: {len(df)} registros\")\n",
        "    return df\n",
        "\n",
        "# Cargar datos\n",
        "df = load_processed_data()\n",
        "\n",
        "# Mostrar informaci√≥n b√°sica\n",
        "print(f\"\\nüìã Informaci√≥n del dataset:\")\n",
        "print(f\"Forma: {df.shape}\")\n",
        "print(f\"Columnas: {list(df.columns)}\")\n",
        "print(f\"\\nüéØ Variable objetivo (churn): {df['churn'].value_counts().to_dict()}\")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîç An√°lisis Exploratorio para ML"
      ],
      "metadata": {
        "id": "eda_ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An√°lisis exploratorio espec√≠fico para Machine Learning\n",
        "def exploratory_analysis_ml(df):\n",
        "    print(\"üîç AN√ÅLISIS EXPLORATORIO PARA MACHINE LEARNING\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # 1. Distribuci√≥n de la variable objetivo\n",
        "    churn_dist = df['churn'].value_counts(normalize=True)\n",
        "    print(f\"\\nüìä Distribuci√≥n de Churn:\")\n",
        "    print(f\"No Churn (0): {churn_dist[0]:.2%}\")\n",
        "    print(f\"Churn (1): {churn_dist[1]:.2%}\")\n",
        "    \n",
        "    # 2. Correlaciones importantes\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    correlations = df[numeric_cols].corr()['churn'].sort_values(ascending=False)\n",
        "    print(f\"\\nüîó Top 5 correlaciones con Churn:\")\n",
        "    for i, (col, corr) in enumerate(correlations.head().items()):\n",
        "        if col != 'churn':\n",
        "            print(f\"{i+1}. {col}: {corr:.3f}\")\n",
        "    \n",
        "    # 3. Crear visualizaciones\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # Distribuci√≥n de churn\n",
        "    df['churn'].value_counts().plot(kind='bar', ax=axes[0,0], color=['green', 'red'])\n",
        "    axes[0,0].set_title('Distribuci√≥n de Churn')\n",
        "    axes[0,0].set_xlabel('Churn (0=No, 1=S√≠)')\n",
        "    axes[0,0].set_ylabel('Cantidad')\n",
        "    \n",
        "    # Churn por tipo de plan\n",
        "    churn_by_plan = df.groupby('plan_type')['churn'].mean()\n",
        "    churn_by_plan.plot(kind='bar', ax=axes[0,1], color='orange')\n",
        "    axes[0,1].set_title('Tasa de Churn por Tipo de Plan')\n",
        "    axes[0,1].set_xlabel('Tipo de Plan')\n",
        "    axes[0,1].set_ylabel('Tasa de Churn')\n",
        "    \n",
        "    # Distribuci√≥n de tenure por churn\n",
        "    df[df['churn']==0]['tenure_months'].hist(alpha=0.7, label='No Churn', ax=axes[1,0], color='green')\n",
        "    df[df['churn']==1]['tenure_months'].hist(alpha=0.7, label='Churn', ax=axes[1,0], color='red')\n",
        "    axes[1,0].set_title('Distribuci√≥n de Antig√ºedad por Churn')\n",
        "    axes[1,0].set_xlabel('Meses de Antig√ºedad')\n",
        "    axes[1,0].set_ylabel('Frecuencia')\n",
        "    axes[1,0].legend()\n",
        "    \n",
        "    # Heatmap de correlaciones\n",
        "    top_corr_cols = correlations.abs().head(8).index\n",
        "    corr_matrix = df[top_corr_cols].corr()\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1,1])\n",
        "    axes[1,1].set_title('Matriz de Correlaciones (Top Variables)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return correlations\n",
        "\n",
        "# Ejecutar an√°lisis\n",
        "correlations = exploratory_analysis_ml(df)"
      ],
      "metadata": {
        "id": "eda_analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üõ†Ô∏è Preparaci√≥n de Datos para ML"
      ],
      "metadata": {
        "id": "data_prep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparaci√≥n de datos para Machine Learning\n",
        "def prepare_data_for_ml(df):\n",
        "    print(\"üõ†Ô∏è PREPARANDO DATOS PARA MACHINE LEARNING\")\n",
        "    print(\"=\" * 45)\n",
        "    \n",
        "    # Crear una copia para no modificar el original\n",
        "    df_ml = df.copy()\n",
        "    \n",
        "    # 1. Manejo de variables categ√≥ricas\n",
        "    categorical_cols = ['plan_type', 'region', 'customer_segment']\n",
        "    \n",
        "    # Encoding de variables categ√≥ricas usando pd.get_dummies\n",
        "    df_encoded = pd.get_dummies(df_ml, columns=categorical_cols, prefix=categorical_cols, drop_first=True)\n",
        "    \n",
        "    # 2. Selecci√≥n de features relevantes (excluyendo IDs y fechas)\n",
        "    exclude_cols = ['customer_id', 'signup_date']\n",
        "    feature_cols = [col for col in df_encoded.columns if col not in exclude_cols + ['churn']]\n",
        "    \n",
        "    # 3. Preparar X e y\n",
        "    X = df_encoded[feature_cols]\n",
        "    y = df_encoded['churn']\n",
        "    \n",
        "    # 4. Verificar y manejar valores faltantes\n",
        "    if X.isnull().sum().sum() > 0:\n",
        "        print(f\"‚ö†Ô∏è Valores faltantes encontrados: {X.isnull().sum().sum()}\")\n",
        "        X = X.fillna(X.median())\n",
        "    \n",
        "    print(f\"‚úÖ Features preparadas: {X.shape[1]} variables\")\n",
        "    print(f\"üìä Muestras totales: {X.shape[0]}\")\n",
        "    print(f\"üéØ Distribuci√≥n objetivo: {y.value_counts().to_dict()}\")\n",
        "    \n",
        "    return X, y, feature_cols\n",
        "\n",
        "# Preparar datos\n",
        "X, y, feature_names = prepare_data_for_ml(df)\n",
        "\n",
        "# Mostrar informaci√≥n de las features\n",
        "print(f\"\\nüìã Features utilizadas:\")\n",
        "for i, feature in enumerate(feature_names, 1):\n",
        "    print(f\"{i:2d}. {feature}\")\n",
        "\n",
        "# Dividir en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nüîÑ Divisi√≥n de datos:\")\n",
        "print(f\"Entrenamiento: {X_train.shape[0]} muestras\")\n",
        "print(f\"Prueba: {X_test.shape[0]} muestras\")\n",
        "\n",
        "# Normalizaci√≥n de features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"‚úÖ Normalizaci√≥n aplicada\")"
      ],
      "metadata": {
        "id": "data_preparation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Entrenamiento de Modelos"
      ],
      "metadata": {
        "id": "model_training"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funci√≥n para entrenar y evaluar m√∫ltiples modelos\n",
        "def train_multiple_models(X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled):\n",
        "    print(\"ü§ñ ENTRENANDO M√öLTIPLES MODELOS\")\n",
        "    print(\"=\" * 35)\n",
        "    \n",
        "    # Definir modelos\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "        'SVM': SVC(random_state=42, probability=True)\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nüîÑ Entrenando {name}...\")\n",
        "        \n",
        "        # Usar datos escalados para modelos que lo requieren\n",
        "        if name in ['Logistic Regression', 'SVM']:\n",
        "            model.fit(X_train_scaled, y_train)\n",
        "            y_pred = model.predict(X_test_scaled)\n",
        "            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        \n",
        "        # Calcular m√©tricas\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        \n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'auc': auc,\n",
        "            'y_pred': y_pred,\n",
        "            'y_pred_proba': y_pred_proba\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ {name} completado\")\n",
        "        print(f\"   Accuracy: {accuracy:.3f}\")\n",
        "        print(f\"   F1-Score: {f1:.3f}\")\n",
        "        print(f\"   AUC: {auc:.3f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Entrenar modelos\n",
        "model_results = train_multiple_models(X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled)"
      ],
      "metadata": {
        "id": "model_training_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Evaluaci√≥n y Comparaci√≥n de Modelos"
      ],
      "metadata": {
        "id": "model_evaluation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funci√≥n para comparar modelos y crear visualizaciones\n",
        "def evaluate_and_compare_models(results, y_test):\n",
        "    print(\"üìä EVALUACI√ìN Y COMPARACI√ìN DE MODELOS\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Crear DataFrame con m√©tricas\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Model': list(results.keys()),\n",
        "        'Accuracy': [results[model]['accuracy'] for model in results.keys()],\n",
        "        'Precision': [results[model]['precision'] for model in results.keys()],\n",
        "        'Recall': [results[model]['recall'] for model in results.keys()],\n",
        "        'F1-Score': [results[model]['f1'] for model in results.keys()],\n",
        "        'AUC': [results[model]['auc'] for model in results.keys()]\n",
        "    })\n",
        "    \n",
        "    # Ordenar por F1-Score\n",
        "    metrics_df = metrics_df.sort_values('F1-Score', ascending=False)\n",
        "    \n",
        "    print(\"\\nüèÜ RANKING DE MODELOS (por F1-Score):\")\n",
        "    print(metrics_df.round(3).to_string(index=False))\n",
        "    \n",
        "    # Mejor modelo\n",
        "    best_model_name = metrics_df.iloc[0]['Model']\n",
        "    print(f\"\\nü•á Mejor modelo: {best_model_name}\")\n",
        "    \n",
        "    # Crear visualizaciones\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # 1. Comparaci√≥n de m√©tricas\n",
        "    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
        "    x_pos = np.arange(len(results))\n",
        "    \n",
        "    ax = axes[0,0]\n",
        "    width = 0.15\n",
        "    for i, metric in enumerate(metrics_to_plot):\n",
        "        values = [results[model][metric.lower().replace('-', '_')] for model in results.keys()]\n",
        "        ax.bar(x_pos + i*width, values, width, label=metric)\n",
        "    \n",
        "    ax.set_xlabel('Modelos')\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Comparaci√≥n de M√©tricas por Modelo')\n",
        "    ax.set_xticks(x_pos + width * 2)\n",
        "    ax.set_xticklabels(list(results.keys()), rotation=45)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Matriz de confusi√≥n del mejor modelo\n",
        "    best_y_pred = results[best_model_name]['y_pred']\n",
        "    cm = confusion_matrix(y_test, best_y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,1])\n",
        "    axes[0,1].set_title(f'Matriz de Confusi√≥n - {best_model_name}')\n",
        "    axes[0,1].set_xlabel('Predicci√≥n')\n",
        "    axes[0,1].set_ylabel('Real')\n",
        "    \n",
        "    # 3. Curvas ROC\n",
        "    ax = axes[1,0]\n",
        "    for name in results.keys():\n",
        "        fpr, tpr, _ = roc_curve(y_test, results[name]['y_pred_proba'])\n",
        "        auc_score = results[name]['auc']\n",
        "        ax.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')\n",
        "    \n",
        "    ax.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "    ax.set_xlabel('Tasa de Falsos Positivos')\n",
        "    ax.set_ylabel('Tasa de Verdaderos Positivos')\n",
        "    ax.set_title('Curvas ROC - Comparaci√≥n de Modelos')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. F1-Score por modelo\n",
        "    f1_scores = [results[model]['f1'] for model in results.keys()]\n",
        "    colors = ['gold' if model == best_model_name else 'skyblue' for model in results.keys()]\n",
        "    axes[1,1].bar(list(results.keys()), f1_scores, color=colors)\n",
        "    axes[1,1].set_title('F1-Score por Modelo')\n",
        "    axes[1,1].set_ylabel('F1-Score')\n",
        "    axes[1,1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Agregar valores en las barras\n",
        "    for i, v in enumerate(f1_scores):\n",
        "        axes[1,1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return metrics_df, best_model_name\n",
        "\n",
        "# Evaluar modelos\n",
        "metrics_comparison, best_model = evaluate_and_compare_models(model_results, y_test)"
      ],
      "metadata": {
        "id": "model_evaluation_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîç An√°lisis de Feature Importance"
      ],
      "metadata": {
        "id": "feature_importance"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An√°lisis de importancia de caracter√≠sticas\n",
        "def analyze_feature_importance(results, feature_names, best_model_name):\n",
        "    print(\"üîç AN√ÅLISIS DE IMPORTANCIA DE CARACTER√çSTICAS\")\n",
        "    print(\"=\" * 45)\n",
        "    \n",
        "    # Obtener el mejor modelo\n",
        "    best_model_obj = results[best_model_name]['model']\n",
        "    \n",
        "    # Extraer importancia de caracter√≠sticas seg√∫n el tipo de modelo\n",
        "    if hasattr(best_model_obj, 'feature_importances_'):\n",
        "        # Para Random Forest y Gradient Boosting\n",
        "        importance = best_model_obj.feature_importances_\n",
        "        importance_type = \"Feature Importance\"\n",
        "    elif hasattr(best_model_obj, 'coef_'):\n",
        "        # Para Logistic Regression\n",
        "        importance = np.abs(best_model_obj.coef_[0])\n",
        "        importance_type = \"Coeficientes (Valor Absoluto)\"\n",
        "    else:\n",
        "        print(\"‚ùå El modelo seleccionado no soporta an√°lisis de importancia\")\n",
        "        return\n",
        "    \n",
        "    # Crear DataFrame con importancia\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importance\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "    \n",
        "    print(f\"\\nüìä Top 10 caracter√≠sticas m√°s importantes ({best_model_name}):\")\n",
        "    print(importance_df.head(10).to_string(index=False, float_format='%.4f'))\n",
        "    \n",
        "    # Visualizaci√≥n\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_15 = importance_df.head(15)\n",
        "    \n",
        "    plt.barh(range(len(top_15)), top_15['Importance'], color='steelblue')\n",
        "    plt.yticks(range(len(top_15)), top_15['Feature'])\n",
        "    plt.xlabel(importance_type)\n",
        "    plt.title(f'Top 15 Caracter√≠sticas m√°s Importantes\\n({best_model_name})')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.grid(True, alpha=0.3, axis='x')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return importance_df\n",
        "\n",
        "# An√°lisis de importancia\n",
        "feature_importance = analyze_feature_importance(model_results, feature_names, best_model)"
      ],
      "metadata": {
        "id": "feature_importance_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üéØ Optimizaci√≥n del Mejor Modelo"
      ],
      "metadata": {
        "id": "model_optimization"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizaci√≥n del mejor modelo usando GridSearch\n",
        "def optimize_best_model(best_model_name, X_train, y_train, X_train_scaled):\n",
        "    print(f\"üéØ OPTIMIZACI√ìN DEL MEJOR MODELO: {best_model_name}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Definir par√°metros para optimizaci√≥n seg√∫n el modelo\n",
        "    if best_model_name == 'Random Forest':\n",
        "        model = RandomForestClassifier(random_state=42)\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [10, 20, None],\n",
        "            'min_samples_split': [2, 5],\n",
        "            'min_samples_leaf': [1, 2]\n",
        "        }\n",
        "        X_train_opt = X_train\n",
        "        \n",
        "    elif best_model_name == 'Gradient Boosting':\n",
        "        model = GradientBoostingClassifier(random_state=42)\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'learning_rate': [0.05, 0.1, 0.2],\n",
        "            'max_depth': [3, 5, 7]\n",
        "        }\n",
        "        X_train_opt = X_train\n",
        "        \n",
        "    elif best_model_name == 'Logistic Regression':\n",
        "        model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "        param_grid = {\n",
        "            'C': [0.1, 1, 10, 100],\n",
        "            'penalty': ['l1', 'l2'],\n",
        "            'solver': ['liblinear']\n",
        "        }\n",
        "        X_train_opt = X_train_scaled\n",
        "        \n",
        "    elif best_model_name == 'SVM':\n",
        "        model = SVC(random_state=42, probability=True)\n",
        "        param_grid = {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'kernel': ['rbf', 'linear'],\n",
        "            'gamma': ['scale', 'auto']\n",
        "        }\n",
        "        X_train_opt = X_train_scaled\n",
        "    \n",
        "    print(f\"üîÑ Iniciando GridSearch para {best_model_name}...\")\n",
        "    print(f\"üìä Par√°metros a probar: {len(param_grid)} hiperpar√°metros\")\n",
        "    \n",
        "    # GridSearch con validaci√≥n cruzada\n",
        "    grid_search = GridSearchCV(\n",
        "        model, \n",
        "        param_grid, \n",
        "        cv=5, \n",
        "        scoring='f1',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    grid_search.fit(X_train_opt, y_train)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Optimizaci√≥n completada\")\n",
        "    print(f\"üèÜ Mejor F1-Score (CV): {grid_search.best_score_:.4f}\")\n",
        "    print(f\"üéõÔ∏è Mejores par√°metros:\")\n",
        "    for param, value in grid_search.best_params_.items():\n",
        "        print(f\"   {param}: {value}\")\n",
        "    \n",
        "    return grid_search.best_estimator_, grid_search.best_params_\n",
        "\n",
        "# Optimizar el mejor modelo\n",
        "optimized_model, best_params = optimize_best_model(best_model, X_train, y_train, X_train_scaled)"
      ],
      "metadata": {
        "id": "model_optimization_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìà Predicciones y An√°lisis Final"
      ],
      "metadata": {
        "id": "final_predictions"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluaci√≥n final del modelo optimizado\n",
        "def final_model_evaluation(optimized_model, best_model_name, X_test, y_test, X_test_scaled):\n",
        "    print(\"üìà EVALUACI√ìN FINAL DEL MODELO OPTIMIZADO\")\n",
        "    print(\"=\" * 42)\n",
        "    \n",
        "    # Usar datos apropiados seg√∫n el modelo\n",
        "    if best_model_name in ['Logistic Regression', 'SVM']:\n",
        "        X_test_final = X_test_scaled\n",
        "    else:\n",
        "        X_test_final = X_test\n",
        "    \n",
        "    # Predicciones\n",
        "    y_pred_final = optimized_model.predict(X_test_final)\n",
        "    y_pred_proba_final = optimized_model.predict_proba(X_test_final)[:, 1]\n",
        "    \n",
        "    # M√©tricas finales\n",
        "    accuracy_final = accuracy_score(y_test, y_pred_final)\n",
        "    precision_final = precision_score(y_test, y_pred_final)\n",
        "    recall_final = recall_score(y_test, y_pred_final)\n",
        "    f1_final = f1_score(y_test, y_pred_final)\n",
        "    auc_final = roc_auc_score(y_test, y_pred_proba_final)\n",
        "    \n",
        "    print(f\"\\nüéØ M√âTRICAS DEL MODELO OPTIMIZADO:\")\n",
        "    print(f\"Accuracy:  {accuracy_final:.4f}\")\n",
        "    print(f\"Precision: {precision_final:.4f}\")\n",
        "    print(f\"Recall:    {recall_final:.4f}\")\n",
        "    print(f\"F1-Score:  {f1_final:.4f}\")\n",
        "    print(f\"AUC:       {auc_final:.4f}\")\n",
        "    \n",
        "    # Reporte de clasificaci√≥n detallado\n",
        "    print(f\"\\nüìä REPORTE DE CLASIFICACI√ìN DETALLADO:\")\n",
        "    print(classification_report(y_test, y_pred_final, target_names=['No Churn', 'Churn']))\n",
        "    \n",
        "    # An√°lisis de predicciones\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'Real': y_test,\n",
        "        'Predicci√≥n': y_pred_final,\n",
        "        'Probabilidad_Churn': y_pred_proba_final\n",
        "    })\n",
        "    \n",
        "    # An√°lisis de casos\n",
        "    true_positives = len(predictions_df[(predictions_df['Real'] == 1) & (predictions_df['Predicci√≥n'] == 1)])\n",
        "    false_positives = len(predictions_df[(predictions_df['Real'] == 0) & (predictions_df['Predicci√≥n'] == 1)])\n",
        "    true_negatives = len(predictions_df[(predictions_df['Real'] == 0) & (predictions_df['Predicci√≥n'] == 0)])\n",
        "    false_negatives = len(predictions_df[(predictions_df['Real'] == 1) & (predictions_df['Predicci√≥n'] == 0)])\n",
        "    \n",
        "    print(f\"\\nüîç AN√ÅLISIS DE PREDICCIONES:\")\n",
        "    print(f\"Verdaderos Positivos (Churn detectado correctamente): {true_positives}\")\n",
        "    print(f\"Falsos Positivos (Falsa alarma de churn): {false_positives}\")\n",
        "    print(f\"Verdaderos Negativos (No churn detectado correctamente): {true_negatives}\")\n",
        "    print(f\"Falsos Negativos (Churn no detectado): {false_negatives}\")\n",
        "    \n",
        "    # Clientes de alto riesgo\n",
        "    high_risk_threshold = 0.7\n",
        "    high_risk_customers = predictions_df[predictions_df['Probabilidad_Churn'] >= high_risk_threshold]\n",
        "    \n",
        "    print(f\"\\n‚ö†Ô∏è CLIENTES DE ALTO RIESGO (>= {high_risk_threshold:.0%} probabilidad):\")\n",
        "    print(f\"Total de clientes de alto riesgo: {len(high_risk_customers)}\")\n",
        "    print(f\"Porcentaje del total: {len(high_risk_customers)/len(predictions_df):.2%}\")\n",
        "    \n",
        "    return predictions_df, {\n",
        "        'accuracy': accuracy_final,\n",
        "        'precision': precision_final,\n",
        "        'recall': recall_final,\n",
        "        'f1': f1_final,\n",
        "        'auc': auc_final\n",
        "    }\n",
        "\n",
        "# Evaluaci√≥n final\n",
        "final_predictions, final_metrics = final_model_evaluation(\n",
        "    optimized_model, best_model, X_test, y_test, X_test_scaled\n",
        ")\n",
        "\n",
        "# Guardar predicciones\n",
        "final_predictions.to_csv('churn_predictions.csv', index=False)\n",
        "print(f\"\\nüíæ Predicciones guardadas en: churn_predictions.csv\")"
      ],
      "metadata": {
        "id": "final_evaluation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÑ Informe Final de Modelos Predictivos"
      ],
      "metadata": {
        "id": "final_report"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Informe final completo\n",
        "def generate_final_report(best_model, final_metrics, feature_importance, best_params):\n",
        "    print(\"=\" * 70)\n",
        "    print(\"üìÑ INFORME FINAL - MODELOS PREDICTIVOS TELECOMX\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    print(f\"\\nüéØ OBJETIVO CUMPLIDO:\")\n",
        "    print(f\"‚úÖ Desarrollo de modelos predictivos para churn de clientes\")\n",
        "    print(f\"‚úÖ Identificaci√≥n de factores clave en el abandono\")\n",
        "    print(f\"‚úÖ Optimizaci√≥n y validaci√≥n de modelos\")\n",
        "    \n",
        "    print(f\"\\nüèÜ MEJOR MODELO SELECCIONADO: {best_model}\")\n",
        "    print(f\"\\nüìä RENDIMIENTO DEL MODELO OPTIMIZADO:\")\n",
        "    for metric, value in final_metrics.items():\n",
        "        emoji = \"üéØ\" if metric == 'f1' else \"üìà\"\n",
        "        print(f\"{emoji} {metric.upper()}: {value:.4f}\")\n",
        "    \n",
        "    print(f\"\\nüîß PAR√ÅMETROS OPTIMIZADOS:\")\n",
        "    for param, value in best_params.items():\n",
        "        print(f\"   ‚Ä¢ {param}: {value}\")\n",
        "    \n",
        "    print(f\"\\nüîç TOP 5 FACTORES PREDICTIVOS:\")\n",
        "    top_5_features = feature_importance.head(5)\n",
        "    for i, (_, row) in enumerate(top_5_features.iterrows(), 1):\n",
        "        print(f\"{i}. {row['Feature']}: {row['Importance']:.4f}\")\n",
        "    \n",
        "    print(f\"\\nüí° INSIGHTS CLAVE:\")\n",
        "    print(f\"‚Ä¢ El modelo puede predecir churn con {final_metrics['accuracy']:.1%} de precisi√≥n\")\n",
        "    print(f\"‚Ä¢ Identifica correctamente {final_metrics['recall']:.1%} de los casos de churn\")\n",
        "    print(f\"‚Ä¢ {final_metrics['precision']:.1%} de las predicciones de churn son correctas\")\n",
        "    print(f\"‚Ä¢ AUC de {final_metrics['auc']:.3f} indica excelente capacidad discriminatoria\")\n",
        "    \n",
        "    print(f\"\\nüéØ RECOMENDACIONES DE NEGOCIO:\")\n",
        "    print(f\"1. üö® PREVENCI√ìN PROACTIVA:\")\n",
        "    print(f\"   - Implementar alertas autom√°ticas para clientes de alto riesgo\")\n",
        "    print(f\"   - Crear campa√±as de retenci√≥n personalizadas\")\n",
        "    \n",
        "    print(f\"\\n2. üìä MONITOREO CONTINUO:\")\n",
        "    print(f\"   - Evaluar modelo mensualmente con nuevos datos\")\n",
        "    print(f\"   - Ajustar umbrales seg√∫n costo de retenci√≥n vs p√©rdida\")\n",
        "    \n",
        "    print(f\"\\n3. üéØ SEGMENTACI√ìN INTELIGENTE:\")\n",
        "    print(f\"   - Usar factores predictivos para segmentar ofertas\")\n",
        "    print(f\"   - Personalizar experiencia basada en riesgo de churn\")\n",
        "    \n",
        "    print(f\"\\n4. üí∞ IMPACTO FINANCIERO:\")\n",
        "    print(f\"   - Priorizar retenci√≥n en clientes de alto valor\")\n",
        "    print(f\"   - Calcular ROI de campa√±as de retenci√≥n\")\n",
        "    \n",
        "    print(f\"\\nüîÆ PR√ìXIMOS PASOS:\")\n",
        "    print(f\"‚Ä¢ Implementar modelo en producci√≥n\")\n",
        "    print(f\"‚Ä¢ Crear pipeline automatizado de reentrenamiento\")\n",
        "    print(f\"‚Ä¢ Desarrollar dashboard de monitoreo en tiempo real\")\n",
        "    print(f\"‚Ä¢ A/B testing de estrategias de retenci√≥n\")\n",
        "    print(f\"‚Ä¢ Incorporar nuevas fuentes de datos (comportamiento web, soporte)\")\n",
        "    \n",
        "    print(f\"\\nüìà BENEFICIOS ESPERADOS:\")\n",
        "    print(f\"‚Ä¢ Reducci√≥n estimada del churn: 15-25%\")\n",
        "    print(f\"‚Ä¢ Mejora en la eficiencia de campa√±as de retenci√≥n: +30%\")\n",
        "    print(f\"‚Ä¢ Incremento en CLV (Customer Lifetime Value): +20%\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚úÖ PROYECTO DE MODELOS PREDICTIVOS COMPLETADO EXITOSAMENTE\")\n",
        "    print(\"üöÄ ¬°LISTO PARA IMPLEMENTACI√ìN EN PRODUCCI√ìN!\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "# Generar informe final\n",
        "generate_final_report(best_model, final_metrics, feature_importance, best_params)"
      ],
      "metadata": {
        "id": "final_report_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üíæ Guardar Modelo y Resultados"
      ],
      "metadata": {
        "id": "save_model"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar modelo y objetos importantes\n",
        "import pickle\n",
        "\n",
        "def save_model_and_artifacts(optimized_model, scaler, feature_names, best_model_name):\n",
        "    print(\"üíæ GUARDANDO MODELO Y ARTEFACTOS\")\n",
        "    print(\"=\" * 35)\n",
        "    \n",
        "    # Crear diccionario con todos los artefactos\n",
        "    model_artifacts = {\n",
        "        'model': optimized_model,\n",
        "        'scaler': scaler,\n",
        "        'feature_names': feature_names,\n",
        "        'model_name': best_model_name,\n",
        "        'metrics': final_metrics,\n",
        "        'best_params': best_params\n",
        "    }\n",
        "    \n",
        "    # Guardar modelo\n",
        "    with open('telecomx_churn_model.pkl', 'wb') as f:\n",
        "        pickle.dump(model_artifacts, f)\n",
        "    \n",
        "    print(f\"‚úÖ Modelo guardado como: telecomx_churn_model.pkl\")\n",
        "    print(f\"üéØ Modelo: {best_model_name}\")\n",
        "    print(f\"üìä F1-Score: {final_metrics['f1']:.4f}\")\n",
        "    print(f\"üìà AUC: {final_metrics['auc']:.4f}\")\n",
        "    \n",
        "    # Funci√≥n de ejemplo para usar el modelo\n",
        "    print(f\"\\nüîß EJEMPLO DE USO DEL MODELO:\")\n",
        "    print(\"\"\"\n",
        "# Para cargar y usar el modelo:\n",
        "import pickle\n",
        "\n",
        "# Cargar modelo\n",
        "with open('telecomx_churn_model.pkl', 'rb') as f:\n",
        "    artifacts = pickle.load(f)\n",
        "\n",
        "model = artifacts['model']\n",
        "scaler = artifacts['scaler']\n",
        "feature_names = artifacts['feature_names']\n",
        "\n",
        "# Hacer predicci√≥n (ejemplo con nuevos datos)\n",
        "# new_data = pd.DataFrame(...) # Nuevos datos con mismas columnas\n",
        "# new_data_scaled = scaler.transform(new_data)\n",
        "# prediction = model.predict(new_data_scaled)\n",
        "# probability = model.predict_proba(new_data_scaled)[:, 1]\n",
        "    \"\"\")\n",
        "\n",
        "# Guardar modelo\n",
        "save_model_and_artifacts(optimized_model, scaler, feature_names, best_model)\n",
        "\n",
        "print(f\"\\nüéâ ¬°PROYECTO COMPLETADO!\")\n",
        "print(f\"üìÅ Archivos generados:\")\n",
        "print(f\"   ‚Ä¢ TelecomX_Predictive_Models.ipynb (este notebook)\")\n",
        "print(f\"   ‚Ä¢ churn_predictions.csv (predicciones del modelo)\")\n",
        "print(f\"   ‚Ä¢ telecomx_churn_model.pkl (modelo entrenado)\")"
      ],
      "metadata": {
        "id": "save_model_code"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
